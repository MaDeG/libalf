\chapter{Learning Algorithms}

A learning algorithm is a component that retrieves the desired information from the knowledgebase to construct a conjecture. 
A conventional way to distinguish learning algorithms is to group them into \online and \offline algorithms. Online learning techniques are capable of actively asking queries to some kind of teacher who is able to classify these queries. Offline algorithms, on the other hand, are passively provided with a set of classified examples from which they have to build the conjecture.

\subsection*{Online Algorithms}

 The online learning process is based on the model of Minimally Adequate Teacher (MAT) wherein the technique invloves two types of queries, namely, \emph{membership queries} and \emph{equivalence queries}. Online learning algorithms build the conjecture by actively asking queries to a \teacher (i.e.\ a user application). \\ The teacher is required to \emph{resolve} the memebership query by providing the classification of the given word. Equivalence queries check whether a derived conjecture is an equivalent description of the target language to be inferred.
\subsection*{Example Code}

\lstset{language=c++, numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt}
\begin{lstlisting}[frame=single]
void main(int argc, char**argv) {
int alphabetsize = get_AlphabetSize();
knowledgebase<bool> base;
angluin_simple_table<bool> algorithm(&base,
NULL,alphabetsize);
do {
 conjecture * cj = algorithm.advance();
 if (cj == NULL) 
 {
    // Get queries from the knowledgebase and 
    // present it to the user to resolve.
 }
 else 
 {
    // Get the conjecture and present it to the user. 
    // If conjecture incorrect, get counter example
 }
}while (result == NULL);
}
\end{lstlisting}


\begin{itemize}
 \item The algorithm runs on iteration which begins at making an \emph{advance} where in the algorithm tries to compute a conjecture with information available in the knowledgebase.
 \item This leads to the rise of membership queries if no conjecture was created. These queries are resolved by the teacher, answer added to the knowledgebase and the algorithm continues the iteration.
 \item On the other hand, if a conjecture was computed, it is presented to the teacher. The algorithm terminates if the conjecture is correct. Otherwise, the iteration continues after the teacher renders a counter example.
\end{itemize}

\subsection*{Offline Algorithms}

Offline algorithms, in contrast to the online variant, is an NP complete problem of finding the smallest DFA consistant with a given set of classified words.
The algorithm is provided with a set S of classified words (called samples) and the algorithm derives a conjecture which conforms to these samples. 

\paragraph{}
The working of this algorithm follows a simple two step procedure. 
\begin{itemize}
 \item The knowledgebase is furnished with all samples. 
 \item The algorithm is then made to advance to compute the conjecture conforming to the samples. 
\end{itemize}

\subsection*{List of Algorithms}

As of \today, \libalf implements seven algorithms for both deterministic and non deterministic automata as listed below.

\paragraph{Online Algorithms}

\begin{enumerate}
 \item \textbf{Angluin L\textsuperscript{*}} \cite{36889-angluin1} \cite{181015-angluin2} \cite{640230-angluin3}
	(..)
 \item \textbf{NL\textsuperscript{*}} \cite{DBLP:conf/ijcai/BolligHKL09-nl1} \cite{DBLP:conf/wia/GarciaRCA05-nl2}
	(..)
 \item \textbf{Kearns/Vazirani} \cite{DBLP:conf/nips/CrammerKW06-Kearns1} \cite{DBLP:conf/nips/LittmanKS01-kearns2} \cite{200548-kearns3}
	(..)
\end{enumerate}

\paragraph{Offline Algorithms}

\begin{enumerate}
 \item \textbf{RPNI} \cite{DBLP:conf/aia/Hoffmann07-rpni1} \cite{599647-rpni2} \cite{655948-rpni3} \cite{1434324-rpni4}
	(..)
 \item \textbf{Biermann} \cite{17952-biermann1}
	(..)
 \item \textbf{Delete2} \cite{982366-delete1}
	(..)
\end{enumerate}

\section{Methods - User Perspective}

In this section, we describe methods that are important for using \libalf. The following material elaborates on initializing the learning algorithm, its association with knolwedgebase and building the conjecture. Like the knowledgebase, learning algorithm also supports serialization and deserialization. Other methods that enable the user to work on the statistics are also described.

\subsection*{Initalizing the Learning Algorithm}

\begin{itemize}
 \item \textbf{learning\_algorithm(\emph{parameters})} \vskip 1pt
	It initializes the essentials for a learning algorithm such as, pointer to the knowledgebase, setting the alphabet size, normalizer and logger. \\ Furthermore, the features associated with following variables may prove useful. \\
	do\_timing - to create timing statistics this has to be set true \\
	in\_timing - variable to check if the currently timing statistics are being measured. \\
\end{itemize}

For instance, the constructor of an RPNI algorithm is written as follows. \vskip 1pt
\textbf{RPNI(knowledgebase$<$answer$>$ * base, logger * log, int alphabet\_size)} \vskip 1pt
The constructor of all the learning algorithms follow such an initialization. It sets the pointer to the knowledgebase, the logger and the alphabet size. \\
Note: Other algorithms may need more than the above mentioned arguements since it depends on the working of the algorithm itself. However, the above example shows the minimal set of arguements required for any learning algorithm. 

\subsection*{Working with Alphabet Size}

\begin{enumerate}
 \item \textbf{learning\_algorithm::void set\_alphabet\_size(int alphabet\_size)} \vskip 1pt
	The method sets the alphabet size for computing the conjecture. This method is used only during the initial setting of the learning algorithm. 

 \item \textbf{learning\_algorithm::void increase\_alphabet\_size(int new\_asize)} \vskip 1pt
	The method increases the size of the alphabet to a new value.

 \item \textbf{learning\_algorithm::int get\_alphabet\_size()} \vskip 1pt
	Returns the alphabet size of the conjecture.
\end{enumerate}

\subsection*{Knowledgebase in Learning Algorithm}

\begin{enumerate}

 \item \textbf{learning\_algorithm::void set\_knowledge\_source(knowledgebase $<$answer$>$ * base)} \vskip 1pt
	The method sets the source (the knowledgebase) which consists of all the membership information to the learning algorithm.

 \item \textbf{learning\_algorithm::knowledgebase$<$answer$>$ * get\_knowledge\_source()} \vskip 1pt
	Returns the pointer to the knowledgebase which is currently the source of membership information.

\end{enumerate}

\subsection*{Synchronization with Knowledgebas}

As discussed in the previous chapter, the knowledgebase supports \emph{undo} operation. When an undo operation has been performed, the learning alogirthm must be synchronized to the knowledgebase failing to which may generate erroneaous output. We use the following method to synchronize the learning algorithm with the knowledgebase. This method must be called after each undo operation.

\begin{itemize}

 \item \textbf{learning\_algorithm::bool sync\_to\_knowledgebase()} \\
	The method checks the knowledgebase, removes the obsolete knowledge, changes its internal data to be synchronized with the knowledgebase and returns \true. If it returns \false, the algorithm is in undefined state and must not be used antymore. \\
	Note: This method should be called after each undo opearation performed in the knowledgebase.

\end{itemize}

On the other hand, the knowledgebase need not necessarily allow the undo operation. Hence we use the following method to check the same.
\begin{itemize}
 
 \item \textbf{learning\_algorithm::bool supports\_sync()} \\
	 Returns \true if undo operations on the knowledgebase are allowed, otherwise returns \false.

\end{itemize}


\subsection*{Building the Conjecture}

\begin{itemize}
 \item \textbf{learning\_algorithm::conjecture * advance()} \vskip 1pt
	The method returns a conjecture if enough information is available in the knowledgebase to construct one. If not, it returns NULL but produces membership queries that are stored in the knowledgebase.
\end{itemize}

\subsection*{Adding Counter-example}

\begin{itemize}
 \item \textbf{virtual bool add\_counterexample(list$<$int$>$)} \vskip 1pt
	The method is used by \online algorithms when a computed conjecture is declined by the teacher, i.e.\ when the equivalence query is answered negative. The counter example provided is first processed by the learning algorithm which marks it as a membership query and is added to the knowledgebase. \\
	This method is used only by an \online algorithm. For \offline algorithms, this method is a stub.
\end{itemize}

\subsection*{Loggers and Normalizers}

\begin{enumerate}
 \item \textbf{virtual void set\_logger(logger * l)} \vskip 1pt
	If the value of ``l'' is not NULL, then it is set as the logger. Otherwise, the logger is set to ``ignore'' which implies that no logger exists.

 \item \textbf{virtual void set\_normalizer(normalizer * norm)} \vskip 1pt
	Sets the normalizer to the one pointed by the arguement ``norm''.

 \item \textbf{virtual void unset\_normalizer()} \vskip 1pt
	Sets the normailizer to NULL. 
\end{enumerate}

\subsection*{Statistics}

\begin{enumerate}
 \item \textbf{virtual memory\_statistics get\_memory\_statistics()} \vskip 1pt
	Returns the memory statistics.

 \item \textbf{virtual timing\_statistics get\_timing\_statistics()} \vskip 1pt
	Returns the timing statistics which is stored in the variable ``current\_stats''.

 \item \textbf{virtual void enable\_timing()} \vskip 1pt
	Enables the maintenance of timing statistics by setting the ``do\_timing'' variable to be \true.

 \item \textbf{virtual void disable\_timing()} \vskip 1pt
	Disables the maintenance of timing statistics by setting the ``do\_timing'' variable to be \false.

 \item \textbf{virtual void reset\_timing()} \vskip 1pt
	The ``current\_stats'' is reset.
\end{enumerate}



\subsection*{Serialize and Deserialize}

\begin{enumerate}
 \item \textbf{virtual basic\_string$<$int32\_t$>$ serialize()} \vskip 1pt
	The method returns a \stringtype composed of \integer containing the serialization of the state of the learning algorithm.

 \item \textbf{virtual bool deserialize(basic\_string$<$int32\_t$>$::iterator \& it, basic\_string$<$int32\_t$>$::iterator limit)} \vskip 1pt
	Restores the data of a serialized learning algorithm. The current state of the learning algorithms is discarded. \\
	The method returns \true if the deserialization was successfull. Otherwise, returns \false.
\end{enumerate}


\section{Methods - Developer's Perspective}

A developer's perspective of learning algorithm mainly centers on the how the algorithm advances and builds the conjecture. The section describes ``advance()'' and its associated methods.

\begin{enumerate}
 \item \textbf{virtual conjecture * advance()} \vskip 1pt
	The most important method towards building the conjecture. The method first gathers all knowledge available and tries to derive a conjecture. If a conjecture is formulated, it is returned. If a conjecture was not produced, the knowledge having unknown classification is marked \emph{required} and NULL is returned. \\
	The method uses the following methods for its operation.

 \item \textbf{virtual bool complete()} \vskip 1pt
	The method is used by the learning algorithm to complete the table in such a way that a conjecture can be derived from it. Returns true if the table is complete. Returns \false if the table is incomplete due to missing knowledge. 

 \item \textbf{virtual conjecture * derive\_conjecture()} \vskip 1pt
	The methodd derives a conjecture from the given data structure available in the knowledgebase. 

 \item \textbf{virtual bool conjecture\_ready()} \vskip 1pt
	Returns \true if a conjecture can be constructed without any further queries. Otherwise, returns \false.
\end{enumerate}






