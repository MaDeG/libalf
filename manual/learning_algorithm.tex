\chapter{Learning Algorithms}

Learning algorithms try to construct a conjecture from available information stored in the knowledgebase. 
A conventional way to distinguish learning algorithms is to group them into \online and \offline algorithms. Online learning techniques are capable of actively asking queries to some kind of teacher who is able to classify these queries. Offline algorithms, on the other hand, are passively provided with a set of classified examples from which they have to build the conjecture.

\subsection*{Online Algorithms}

 The online learning algorithm follows the most common model of Minimally Adequate Teacher (MAT) that involves some kind of a teacher to  \emph{resolve} two types of queries \emph{membership queries} and \emph{equivalence queries}. Online learning algorithms build the conjecture by actively asking queries to a \teacher (i.e.\ a user application). \\ The teacher is required to resolve a membership query by providing the classification of the given word. Equivalence queries check whether a derived conjecture is an equivalent description of the target language to be inferred.

\begin{itemize}
 \item The algorithm runs on iteration which begins at making an \emph{advance} where in the algorithm tries to compute a conjecture with information available in the knowledgebase.
 \item This leads to the rise of membership queries if no conjecture was created. These queries are resolved by the teacher, answer added to the knowledgebase and the algorithm continues the iteration.
 \item On the other hand, if a conjecture was computed, it is presented to the teacher. The algorithm terminates if the conjecture is correct. Otherwise, the iteration continues after the teacher renders a counter example.
\end{itemize}

\subsection*{Offline Algorithms}    

Offline algorithms, in contrast to the online variant, finds the smallest DFA consistent with a given set of classified words.
The algorithm is provided with a set S of classified words (called samples) and the algorithm derives a conjecture which conforms to these samples. 

\paragraph{}
The working of this algorithm follows a simple two step procedure. 
\begin{itemize}
 \item The knowledgebase is furnished with all samples. 
 \item The algorithm is then made to advance to compute the conjecture conforming to the samples. 
\end{itemize}

\subsection*{List of Algorithms}

As of \today, \libalf implements seven algorithms for both deterministic and non deterministic automata as listed below.

\paragraph{Online Algorithms}

\begin{enumerate}
 \item \textbf{Angluin L\textsuperscript{*}} \cite{36889-angluin1} \cite{181015-angluin2} \cite{640230-angluin3}
	(..)
 \item \textbf{NL\textsuperscript{*}} \cite{DBLP:conf/ijcai/BolligHKL09-nl1} \cite{DBLP:conf/wia/GarciaRCA05-nl2}
	(..)
 \item \textbf{Kearns/Vazirani} \cite{DBLP:conf/nips/CrammerKW06-Kearns1} \cite{DBLP:conf/nips/LittmanKS01-kearns2} \cite{200548-kearns3}
	(..)
\end{enumerate}

\paragraph{Offline Algorithms}

\begin{enumerate}
 \item \textbf{RPNI} \cite{DBLP:conf/aia/Hoffmann07-rpni1} \cite{599647-rpni2} \cite{655948-rpni3} \cite{1434324-rpni4}
	(..)
 \item \textbf{Biermann} \cite{17952-biermann1}
	(..)
 \item \textbf{DeLeTe2} \cite{982366-delete1}
	(..)
\end{enumerate}

\section{Methods - User Perspective}

In this section, we describe methods that are important for using \libalf. The following material elaborates on initializing the learning algorithm, its association with knowledgebase and building the conjecture. Like the knowledgebase, learning algorithm also supports serialization and deserialization. Other methods that enable the user to work on the statistics are also described.

\subsection*{Initializing the Learning Algorithm}
Given below is an example of the RPNI algorithm is initialized. 
\begin{itemize}
 \item \textbf{RPNI(knowledgebase$<$answer$>$ * base, logger * log, int alphabet\_size)} \vskip 1pt
\textbf{learning\_algorithm(\emph{parameters})} \vskip 1pt
	The constructor of all the learning algorithms follow such an initialization. It sets the pointer to the knowledgebase, the logger and the alphabet size. \\
\end{itemize}
Note: Other algorithms may need more than the above mentioned arguments since they depend on the working of the algorithm itself. However, the above example shows the minimal set of arguments required for any learning algorithm. 

\subsection*{Working with Alphabet Size}

\begin{enumerate}
 \item \textbf{learning\_algorithm::void set\_alphabet\_size(int alphabet\_size)} \vskip 1pt
	The method sets the alphabet size for computing the conjecture. This method is used only during the initial setting of the learning algorithm. 

 \item \textbf{learning\_algorithm::void increase\_alphabet\_size(int new\_asize)} \vskip 1pt
	The method increases the size of the alphabet to a new value.

 \item \textbf{learning\_algorithm::int get\_alphabet\_size()} \vskip 1pt
	Returns the alphabet size of the conjecture.
\end{enumerate}

\subsection*{Knowledgebase in Learning Algorithm}

\begin{enumerate}

 \item \textbf{learning\_algorithm::void set\_knowledge\_source(knowledgebase $<$answer$>$ * base)} \vskip 1pt
	The method sets the source (the knowledgebase) which consists of all the membership information to the learning algorithm.

 \item \textbf{learning\_algorithm::knowledgebase$<$answer$>$ * get\_knowledge\_source()} \vskip 1pt
	Returns the pointer to the knowledgebase which is currently the source of membership information.

\end{enumerate}

\subsection*{Advancing}

\begin{itemize}
 \item \textbf{learning\_algorithm::conjecture * advance()} \vskip 1pt
	The method returns a conjecture if enough information is available in the knowledgebase to construct one. If not, it returns NULL but produces membership queries that are stored in the knowledgebase.
\end{itemize}

\subsection*{Adding Counter-example}

\begin{itemize}
 \item \textbf{virtual bool add\_counterexample(list$<$int$>$)} \vskip 1pt
	The method is used by \online algorithms when a computed conjecture is declined by the teacher, i.e.\ when the equivalence query is answered negative. The counter example provided is first processed by the learning algorithm which marks it as a membership query and is added to the knowledgebase. \\
	This method is used only by an \online algorithm. For \offline algorithms, this method is a stub.
\end{itemize}

\subsection*{Synchronization with Knowledgebase}

As discussed in the previous chapter, the knowledgebase supports \emph{undo} operation. When an undo operation has been performed, the learning algorithm must be synchronized to the knowledgebase, failing to which may generate erroneous output. We use the following method to synchronize the learning algorithm with the knowledgebase. This method must be called after each undo operation.

\begin{itemize}

 \item \textbf{learning\_algorithm::bool sync\_to\_knowledgebase()} \\
	The method checks the knowledgebase and changes its internal data to be synchronized with the knowledgebase and returns \true. If it returns \false, the algorithm is in an undefined state and must not be used anymore. \\
	Note: This method should be called after each undo operation performed in the knowledgebase.

\end{itemize}

On the other hand, the knowledgebase need not necessarily allow the undo operation. Hence we use the following method to check the same.
\begin{itemize}
 
 \item \textbf{learning\_algorithm::bool supports\_sync()} \\
	 Returns \true if undo operations on the knowledgebase are allowed, otherwise returns \false.

\end{itemize}


\subsection*{Working with Loggers and Normalizers}

\begin{enumerate}
 \item \textbf{virtual void set\_logger(logger * l)} \vskip 1pt
	If the value of ``l'' is not NULL, then it is set as the logger. Otherwise, the logger is set to ``ignore'' which implies that no logger exists.

 \item \textbf{virtual void set\_normalizer(normalizer * norm)} \vskip 1pt
	Sets the normalizer to the one pointed by the argument ``norm''.

 \item \textbf{virtual void unset\_normalizer()} \vskip 1pt
	Sets the normalizer to NULL. 
\end{enumerate}

\subsection*{Working with Statistics}

\begin{enumerate}
 \item \textbf{virtual memory\_statistics get\_memory\_statistics()} \vskip 1pt
	Returns the memory statistics.

 \item \textbf{virtual timing\_statistics get\_timing\_statistics()} \vskip 1pt
	Returns the timing statistics which is stored in the variable ``current\_stats''.

 \item \textbf{virtual void enable\_timing()} \vskip 1pt
	Enables the maintenance of timing statistics by setting the ``do\_timing'' variable to be \true.

 \item \textbf{virtual void disable\_timing()} \vskip 1pt
	Disables the maintenance of timing statistics by setting the ``do\_timing'' variable to be \false.

 \item \textbf{virtual void reset\_timing()} \vskip 1pt
	The ``current\_stats'' is reset.
\end{enumerate}

\subsection*{Serialize and Deserialize}

\begin{enumerate}
 \item \textbf{virtual basic\_string$<$int32\_t$>$ serialize()} \vskip 1pt
	The method returns a \stringtype composed of \integer containing the serialization of the state of the learning algorithm. The data can be loaded with the following method.

 \item \textbf{virtual bool deserialize(basic\_string$<$int32\_t$>$::iterator \& it, basic\_string $<$int32\_t$>$::iterator limit)} \vskip 1pt
	Restores the data of a serialized learning algorithm. The current state of the learning algorithms is discarded. \\
	The method returns \true if the deserialization was successful. Otherwise, returns \false.
\end{enumerate}


\section{Methods - Developer's Perspective}

A developer's perspective of learning algorithm mainly centers on the how the algorithm advances and builds the conjecture. The section describes ``advance()'' and its associated methods. B

\begin{itemize}
 \item \textbf{virtual conjecture * advance()} \vskip 1pt
	The most important method towards building the conjecture. The method first gathers all knowledge available and tries to derive a conjecture. If a conjecture is formulated, it is returned. If a conjecture was not produced, the knowledge having unknown classification is marked \emph{required} and NULL is returned. \\
\end{itemize}

The following code snippet shows how advance works. The methods used internally are described below.
\begin{lstlisting}
virtual conjecture * advance()
{{{
	conjecture * ret = NULL;
	//When no knowledgebase is found.
	if(my_knowledge == NULL) { 
	  (*my_logger)(LOGGER_ERROR, "learning_algorithm::advance(): no knowledgebase was set!\n");
				return false;
	  }
	 
	start_timing(); // For statistics
	if(complete()) {
	  ret = derive_conjecture();
	     // When a conjecture could not be dervied
	     if(!ret)
	      (*my_logger)(LOGGER_ERROR, "learning_algorithm::advance(): derive from completed data structure failed! possibly internal error.\n");
	  }
	stop_timing(); 
	return ret;
}}}; 
\end{lstlisting}


\begin{enumerate}
 
\item \textbf{virtual bool complete()} \vskip 1pt
	The method is used by the learning algorithm to complete their internal data structures such that a conjecture can be derived from it. Returns \true if all internal data is available. Returns \false if there is missing knowledge. 

 \item \textbf{virtual conjecture * derive\_conjecture()} \vskip 1pt
	The method derives a conjecture from the given data structure available in the knowledgebase. 
\end{enumerate}

One other method used by learning algorithms is the conjecture\_ready method to check if a conjecture can be derived successfuly or not.
\begin{itemize}
  \item \textbf{virtual bool conjecture\_ready()} \vskip 1pt
	Returns \true if a conjecture can be constructed without any further queries. Otherwise, returns \false.
\end{itemize}






